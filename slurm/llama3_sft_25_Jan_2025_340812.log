0: num_nodes: 1
0: SLURM_NODEID: 0
0: CUDA_VISIBLE_DEVICES: 0,1,2,3
0: output_dir: /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_sr_meta-llama-Meta-Llama-3-8B-Instruct_50_500_tbs8_ebs8_gas2_lr5e-6_cl4096
0: [2025-01-25 18:56:14,870] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: [INFO|2025-01-25 18:56:21] llamafactory.cli:157 >> Initializing distributed tasks at: gh034:29501
0: [2025-01-25 18:56:29,615] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-01-25 18:56:29,615] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: [2025-01-25 18:56:30,318] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-01-25 18:56:30,330] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: [2025-01-25 18:56:31,496] [INFO] [comm.py:652:init_distributed] cdb=None
0: [2025-01-25 18:56:31,496] [INFO] [comm.py:652:init_distributed] cdb=None
0: [2025-01-25 18:56:32,198] [INFO] [comm.py:652:init_distributed] cdb=None
0: [2025-01-25 18:56:32,198] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
0: [2025-01-25 18:56:32,219] [INFO] [comm.py:652:init_distributed] cdb=None
0: [INFO|2025-01-25 18:56:32] llamafactory.hparams.parser:359 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|2025-01-25 18:56:32] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|2025-01-25 18:56:32] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|configuration_utils.py:679] 2025-01-25 18:56:32,694 >> loading configuration file config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
0: [INFO|configuration_utils.py:746] 2025-01-25 18:56:32,695 >> Model config LlamaConfig {
0:   "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
0:   "architectures": [
0:     "LlamaForCausalLM"
0:   ],
0:   "attention_bias": false,
0:   "attention_dropout": 0.0,
0:   "bos_token_id": 128000,
0:   "eos_token_id": 128009,
0:   "head_dim": 128,
0:   "hidden_act": "silu",
0:   "hidden_size": 4096,
0:   "initializer_range": 0.02,
0:   "intermediate_size": 14336,
0:   "max_position_embeddings": 8192,
0:   "mlp_bias": false,
0:   "model_type": "llama",
0:   "num_attention_heads": 32,
0:   "num_hidden_layers": 32,
0:   "num_key_value_heads": 8,
0:   "pretraining_tp": 1,
0:   "rms_norm_eps": 1e-05,
0:   "rope_scaling": null,
0:   "rope_theta": 500000.0,
0:   "tie_word_embeddings": false,
0:   "torch_dtype": "bfloat16",
0:   "transformers_version": "4.46.1",
0:   "use_cache": true,
0:   "vocab_size": 128256
0: }
0: 
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:32,922 >> loading file tokenizer.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:32,922 >> loading file tokenizer.model from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:32,922 >> loading file added_tokens.json from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:32,922 >> loading file special_tokens_map.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:32,922 >> loading file tokenizer_config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json
0: [INFO|2025-01-25 18:56:33] llamafactory.hparams.parser:359 >> Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|tokenization_utils_base.py:2475] 2025-01-25 18:56:33,152 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: [INFO|configuration_utils.py:679] 2025-01-25 18:56:33,406 >> loading configuration file config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
0: [INFO|configuration_utils.py:746] 2025-01-25 18:56:33,406 >> Model config LlamaConfig {
0:   "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
0:   "architectures": [
0:     "LlamaForCausalLM"
0:   ],
0:   "attention_bias": false,
0:   "attention_dropout": 0.0,
0:   "bos_token_id": 128000,
0:   "eos_token_id": 128009,
0:   "head_dim": 128,
0:   "hidden_act": "silu",
0:   "hidden_size": 4096,
0:   "initializer_range": 0.02,
0:   "intermediate_size": 14336,
0:   "max_position_embeddings": 8192,
0:   "mlp_bias": false,
0:   "model_type": "llama",
0:   "num_attention_heads": 32,
0:   "num_hidden_layers": 32,
0:   "num_key_value_heads": 8,
0:   "pretraining_tp": 1,
0:   "rms_norm_eps": 1e-05,
0:   "rope_scaling": null,
0:   "rope_theta": 500000.0,
0:   "tie_word_embeddings": false,
0:   "torch_dtype": "bfloat16",
0:   "transformers_version": "4.46.1",
0:   "use_cache": true,
0:   "vocab_size": 128256
0: }
0: 
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:33,457 >> loading file tokenizer.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:33,457 >> loading file tokenizer.model from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:33,457 >> loading file added_tokens.json from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:33,457 >> loading file special_tokens_map.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-25 18:56:33,457 >> loading file tokenizer_config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json
0: [INFO|tokenization_utils_base.py:2475] 2025-01-25 18:56:33,685 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: [INFO|2025-01-25 18:56:33] llamafactory.data.template:157 >> Add pad token: <|eot_id|>
0: [INFO|2025-01-25 18:56:33] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.
0: [WARNING|2025-01-25 18:56:33] llamafactory.data.template:162 >> New tokens have been added, make sure `resize_vocab` is True.
0: [INFO|2025-01-25 18:56:33] llamafactory.data.loader:157 >> Loading dataset beavertails/sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_50_500.json...
0: [rank2]: Traceback (most recent call last):
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
0: [rank2]:     launch()
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
0: [rank2]:     run_exp()
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/tuner.py", line 59, in run_exp
0: [rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
0: [rank2]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 266, in get_dataset
0: [rank2]:     dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 172, in _get_merged_dataset
0: [rank2]:     datasets.append(_load_single_dataset(dataset_attr, model_args, data_args, training_args))
0: [rank2]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 75, in _load_single_dataset
0: [rank2]:     raise ValueError(f"File {local_path} not found.")
0: [rank2]: ValueError: File data/beavertails/sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_50_500.json not found.
0: [rank1]: Traceback (most recent call last):
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
0: [rank1]:     launch()
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
0: [rank1]:     run_exp()
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/tuner.py", line 59, in run_exp
0: [rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
0: [rank1]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 266, in get_dataset
0: [rank1]:     dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 172, in _get_merged_dataset
0: [rank1]:     datasets.append(_load_single_dataset(dataset_attr, model_args, data_args, training_args))
0: [rank1]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 75, in _load_single_dataset
0: [rank1]:     raise ValueError(f"File {local_path} not found.")
0: [rank1]: ValueError: File data/beavertails/sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_50_500.json not found.
0: [rank0]: Traceback (most recent call last):
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
0: [rank0]:     launch()
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
0: [rank0]:     run_exp()
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/tuner.py", line 59, in run_exp
0: [rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
0: [rank0]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 266, in get_dataset
0: [rank0]:     dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 172, in _get_merged_dataset
0: [rank0]:     datasets.append(_load_single_dataset(dataset_attr, model_args, data_args, training_args))
0: [rank0]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 75, in _load_single_dataset
0: [rank0]:     raise ValueError(f"File {local_path} not found.")
0: [rank0]: ValueError: File data/beavertails/sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_50_500.json not found.
0: [rank3]: Traceback (most recent call last):
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
0: [rank3]:     launch()
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
0: [rank3]:     run_exp()
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/tuner.py", line 59, in run_exp
0: [rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
0: [rank3]:     dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 266, in get_dataset
0: [rank3]:     dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 172, in _get_merged_dataset
0: [rank3]:     datasets.append(_load_single_dataset(dataset_attr, model_args, data_args, training_args))
0: [rank3]:   File "/u/cjin1/LLaMA-Factory/src/llamafactory/data/loader.py", line 75, in _load_single_dataset
0: [rank3]:     raise ValueError(f"File {local_path} not found.")
0: [rank3]: ValueError: File data/beavertails/sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_50_500.json not found.
0: [rank0]:[W125 18:56:39.899142373 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
0: W0125 18:56:40.409000 168305 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 168316 closing signal SIGTERM
0: W0125 18:56:40.409000 168305 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 168317 closing signal SIGTERM
0: W0125 18:56:40.409000 168305 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 168318 closing signal SIGTERM
0: E0125 18:56:40.573000 168305 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 0 (pid: 168315) of binary: /usr/bin/python
0: Traceback (most recent call last):
0:   File "/usr/local/bin/torchrun", line 33, in <module>
0:     sys.exit(load_entry_point('torch==2.5.0a0+b465a5843b.nv24.9', 'console_scripts', 'torchrun')())
0:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
0:     return f(*args, **kwargs)
0:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
0:     run(args)
0:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
0:     elastic_launch(
0:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
0:     return launch_agent(self._config, self._entrypoint, list(args))
0:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
0:     raise ChildFailedError(
0: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
0: ============================================================
0: /u/cjin1/LLaMA-Factory/src/llamafactory/launcher.py FAILED
0: ------------------------------------------------------------
0: Failures:
0:   <NO_OTHER_FAILURES>
0: ------------------------------------------------------------
0: Root Cause (first observed failure):
0: [0]:
0:   time      : 2025-01-25_18:56:40
0:   host      : gh034.hsn.cm.delta.internal.ncsa.edu
0:   rank      : 0 (local_rank: 0)
0:   exitcode  : 1 (pid: 168315)
0:   error_file: <N/A>
0:   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
0: ============================================================
0: run_safe_reason/llama3_train_sft.sh: error reading input file: Stale file handle
srun: error: gh034: task 0: Exited with exit code 2
