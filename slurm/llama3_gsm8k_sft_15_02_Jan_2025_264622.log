0: num_nodes: 1
0: SLURM_NODEID: 0
0: CUDA_VISIBLE_DEVICES: 0,1,2,3
0: output_dir: /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048
0: [2025-01-02 11:49:54,739] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: [INFO|2025-01-02 11:50:01] llamafactory.cli:157 >> Initializing distributed tasks at: gh038:29501
0: [2025-01-02 11:50:07,020] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: [2025-01-02 11:50:08,915] [INFO] [comm.py:652:init_distributed] cdb=None
0: [2025-01-02 11:50:10,384] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-01-02 11:50:10,398] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-01-02 11:50:10,400] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: Warning: The cache directory for DeepSpeed Triton autotune, /u/cjin1/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
0: [INFO|2025-01-02 11:50:10] llamafactory.hparams.parser:359 >> Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [2025-01-02 11:50:12,296] [INFO] [comm.py:652:init_distributed] cdb=None
0: [2025-01-02 11:50:12,348] [INFO] [comm.py:652:init_distributed] cdb=None
0: [2025-01-02 11:50:12,348] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
0: [2025-01-02 11:50:12,381] [INFO] [comm.py:652:init_distributed] cdb=None
0: [INFO|2025-01-02 11:50:12] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|configuration_utils.py:679] 2025-01-02 11:50:12,721 >> loading configuration file config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
0: [INFO|configuration_utils.py:746] 2025-01-02 11:50:12,722 >> Model config LlamaConfig {
0:   "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
0:   "architectures": [
0:     "LlamaForCausalLM"
0:   ],
0:   "attention_bias": false,
0:   "attention_dropout": 0.0,
0:   "bos_token_id": 128000,
0:   "eos_token_id": 128009,
0:   "head_dim": 128,
0:   "hidden_act": "silu",
0:   "hidden_size": 4096,
0:   "initializer_range": 0.02,
0:   "intermediate_size": 14336,
0:   "max_position_embeddings": 8192,
0:   "mlp_bias": false,
0:   "model_type": "llama",
0:   "num_attention_heads": 32,
0:   "num_hidden_layers": 32,
0:   "num_key_value_heads": 8,
0:   "pretraining_tp": 1,
0:   "rms_norm_eps": 1e-05,
0:   "rope_scaling": null,
0:   "rope_theta": 500000.0,
0:   "tie_word_embeddings": false,
0:   "torch_dtype": "bfloat16",
0:   "transformers_version": "4.46.1",
0:   "use_cache": true,
0:   "vocab_size": 128256
0: }
0: 
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:12,774 >> loading file tokenizer.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:12,774 >> loading file tokenizer.model from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:12,774 >> loading file added_tokens.json from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:12,774 >> loading file special_tokens_map.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:12,774 >> loading file tokenizer_config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json
0: [INFO|tokenization_utils_base.py:2475] 2025-01-02 11:50:13,008 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: [INFO|configuration_utils.py:679] 2025-01-02 11:50:13,251 >> loading configuration file config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
0: [INFO|configuration_utils.py:746] 2025-01-02 11:50:13,251 >> Model config LlamaConfig {
0:   "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
0:   "architectures": [
0:     "LlamaForCausalLM"
0:   ],
0:   "attention_bias": false,
0:   "attention_dropout": 0.0,
0:   "bos_token_id": 128000,
0:   "eos_token_id": 128009,
0:   "head_dim": 128,
0:   "hidden_act": "silu",
0:   "hidden_size": 4096,
0:   "initializer_range": 0.02,
0:   "intermediate_size": 14336,
0:   "max_position_embeddings": 8192,
0:   "mlp_bias": false,
0:   "model_type": "llama",
0:   "num_attention_heads": 32,
0:   "num_hidden_layers": 32,
0:   "num_key_value_heads": 8,
0:   "pretraining_tp": 1,
0:   "rms_norm_eps": 1e-05,
0:   "rope_scaling": null,
0:   "rope_theta": 500000.0,
0:   "tie_word_embeddings": false,
0:   "torch_dtype": "bfloat16",
0:   "transformers_version": "4.46.1",
0:   "use_cache": true,
0:   "vocab_size": 128256
0: }
0: 
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:13,308 >> loading file tokenizer.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:13,308 >> loading file tokenizer.model from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:13,308 >> loading file added_tokens.json from cache at None
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:13,308 >> loading file special_tokens_map.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json
0: [INFO|tokenization_utils_base.py:2211] 2025-01-02 11:50:13,308 >> loading file tokenizer_config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json
0: [INFO|2025-01-02 11:50:13] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|2025-01-02 11:50:13] llamafactory.hparams.parser:359 >> Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|tokenization_utils_base.py:2475] 2025-01-02 11:50:13,543 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0: [INFO|2025-01-02 11:50:13] llamafactory.data.template:157 >> Add pad token: <|eot_id|>
0: [INFO|2025-01-02 11:50:13] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.
0: [WARNING|2025-01-02 11:50:13] llamafactory.data.template:162 >> New tokens have been added, make sure `resize_vocab` is True.
0: [INFO|2025-01-02 11:50:13] llamafactory.data.loader:157 >> Loading dataset gsm8k/sft.json...
0: [INFO|2025-01-02 11:50:13] llamafactory.data.loader:157 >> Loading dataset gsm8k/sft.json...
0: training example:
0: input_ids:
0: [128000, 128006, 882, 128007, 271, 791, 3430, 315, 459, 12443, 11299, 220, 605, 4, 304, 264, 4040, 2305, 323, 2500, 220, 508, 4, 304, 279, 2132, 2305, 13, 220, 1442, 279, 2926, 3430, 574, 400, 1041, 15, 11, 11294, 279, 3430, 1306, 279, 2132, 2305, 13, 128009, 128006, 78191, 128007, 271, 40, 4265, 387, 6380, 311, 1520, 499, 449, 430, 13, 5810, 596, 279, 6425, 1473, 91084, 11, 279, 3430, 315, 279, 12443, 374, 400, 1041, 15, 382, 644, 279, 1176, 2305, 11, 279, 3430, 17503, 555, 220, 605, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 1041, 15, 482, 320, 605, 4, 315, 400, 1041, 15, 8, 284, 400, 1041, 15, 482, 400, 1041, 284, 400, 7467, 271, 644, 279, 2132, 2305, 11, 279, 3430, 17503, 555, 2500, 220, 508, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 7467, 482, 320, 508, 4, 315, 400, 7467, 8, 284, 400, 7467, 482, 400, 5245, 284, 400, 13104, 271, 10267, 596, 10356, 1057, 22702, 555, 9455, 279, 2860, 11668, 18979, 512, 6475, 3430, 284, 400, 1041, 15, 198, 7117, 1306, 1176, 2305, 284, 400, 7467, 198, 7117, 1306, 213
0: 2, 2305, 284, 400, 13104, 271, 791, 2860, 11668, 18979, 284, 320, 6475, 3430, 482, 8650, 1306, 2132, 2305, 8, 611, 4220, 3430, 198, 28, 1746, 1041, 15, 482, 400, 13104, 8, 611, 400, 1041, 15, 198, 28, 400, 11209, 611, 400, 1041, 15, 198, 28, 220, 1591, 15804, 2028, 9248, 279, 3685, 2860, 11668, 18979, 315, 220, 966, 4, 320, 605, 4, 304, 279, 1176, 2305, 489, 220, 508, 4, 304, 279, 2132, 2305, 3677, 578, 4320, 374, 400, 13104, 13, 128009]
0: inputs:
0: <|begin_of_text|><|start_header_id|>user<|end_header_id|>
0: 
0: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
0: 
0: I'd be happy to help you with that. Here's the solution:
0: 
0: Initially, the price of the iPhone is $1000.
0: 
0: In the first month, the price falls by 10%, which means the new price is:
0: $1000 - (10% of $1000) = $1000 - $100 = $900
0: 
0: In the second month, the price falls by another 20%, which means the new price is:
0: $900 - (20% of $900) = $900 - $180 = $720
0: 
0: Let's verify our calculation by finding the total percentage decrease:
0: Initial price = $1000
0: Price after first month = $900
0: Price after second month = $720
0: 
0: The total percentage decrease = (Initial price - Price after second month) / Initial price
0: = ($1000 - $720) / $1000
0: = $280 / $1000
0: = 28%
0: 
0: This matches the expected total percentage decrease of 30% (10% in the first month + 20% in the second month).
0: 
0:  The answer is $720.<|eot_id|>
0: label_ids:
0: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 4265, 387, 6380, 311, 1520, 499, 449, 430, 13, 5810, 596, 279, 6425, 1473, 91084, 11, 279, 3430, 315, 279, 12443, 374, 400, 1041, 15, 382, 644, 279, 1176, 2305, 11, 279, 3430, 17503, 555, 220, 605, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 1041, 15, 482, 320, 605, 4, 315, 400, 1041, 15, 8, 284, 400, 1041, 15, 482, 400, 1041, 284, 400, 7467, 271, 644, 279, 2132, 2305, 11, 279, 3430, 17503, 555, 2500, 220, 508, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 7467, 482, 320, 508, 4, 315, 400, 7467, 8, 284, 400, 7467, 482, 400, 5245, 284, 400, 13104, 271, 10267, 596, 10356, 1057, 22702, 555, 9455, 279, 2860, 11668, 18979, 512, 6475, 3430, 284, 400, 1041, 15, 198, 7117, 1306, 1176, 2305, 284, 400, 7467, 1
0: 98, 7117, 1306, 2132, 2305, 284, 400, 13104, 271, 791, 2860, 11668, 18979, 284, 320, 6475, 3430, 482, 8650, 1306, 2132, 2305, 8, 611, 4220, 3430, 198, 28, 1746, 1041, 15, 482, 400, 13104, 8, 611, 400, 1041, 15, 198, 28, 400, 11209, 611, 400, 1041, 15, 198, 28, 220, 1591, 15804, 2028, 9248, 279, 3685, 2860, 11668, 18979, 315, 220, 966, 4, 320, 605, 4, 304, 279, 1176, 2305, 489, 220, 508, 4, 304, 279, 2132, 2305, 3677, 578, 4320, 374, 400, 13104, 13, 128009]
0: labels:
0: I'd be happy to help you with that. Here's the solution:
0: 
0: Initially, the price of the iPhone is $1000.
0: 
0: In the first month, the price falls by 10%, which means the new price is:
0: $1000 - (10% of $1000) = $1000 - $100 = $900
0: 
0: In the second month, the price falls by another 20%, which means the new price is:
0: $900 - (20% of $900) = $900 - $180 = $720
0: 
0: Let's verify our calculation by finding the total percentage decrease:
0: Initial price = $1000
0: Price after first month = $900
0: Price after second month = $720
0: 
0: The total percentage decrease = (Initial price - Price after second month) / Initial price
0: = ($1000 - $720) / $1000
0: = $280 / $1000
0: = 28%
0: 
0: This matches the expected total percentage decrease of 30% (10% in the first month + 20% in the second month).
0: 
0:  The answer is $720.<|eot_id|>
0: eval example:
0: input_ids:
0: [128000, 128006, 882, 128007, 271, 791, 3430, 315, 459, 12443, 11299, 220, 605, 4, 304, 264, 4040, 2305, 323, 2500, 220, 508, 4, 304, 279, 2132, 2305, 13, 220, 1442, 279, 2926, 3430, 574, 400, 1041, 15, 11, 11294, 279, 3430, 1306, 279, 2132, 2305, 13, 128009, 128006, 78191, 128007, 271, 40, 4265, 387, 6380, 311, 1520, 499, 449, 430, 13, 5810, 596, 279, 6425, 1473, 91084, 11, 279, 3430, 315, 279, 12443, 374, 400, 1041, 15, 382, 644, 279, 1176, 2305, 11, 279, 3430, 17503, 555, 220, 605, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 1041, 15, 482, 320, 605, 4, 315, 400, 1041, 15, 8, 284, 400, 1041, 15, 482, 400, 1041, 284, 400, 7467, 271, 644, 279, 2132, 2305, 11, 279, 3430, 17503, 555, 2500, 220, 508, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 7467, 482, 320, 508, 4, 315, 400, 7467, 8, 284, 400, 7467, 482, 400, 5245, 284, 400, 13104, 271, 10267, 596, 10356, 1057, 22702, 555, 9455, 279, 2860, 11668, 18979, 512, 6475, 3430, 284, 400, 1041, 15, 198, 7117, 1306, 1176, 2305, 284, 400, 7467, 198, 7117, 1306, 213
0: 2, 2305, 284, 400, 13104, 271, 791, 2860, 11668, 18979, 284, 320, 6475, 3430, 482, 8650, 1306, 2132, 2305, 8, 611, 4220, 3430, 198, 28, 1746, 1041, 15, 482, 400, 13104, 8, 611, 400, 1041, 15, 198, 28, 400, 11209, 611, 400, 1041, 15, 198, 28, 220, 1591, 15804, 2028, 9248, 279, 3685, 2860, 11668, 18979, 315, 220, 966, 4, 320, 605, 4, 304, 279, 1176, 2305, 489, 220, 508, 4, 304, 279, 2132, 2305, 3677, 578, 4320, 374, 400, 13104, 13, 128009]
0: inputs:
0: <|begin_of_text|><|start_header_id|>user<|end_header_id|>
0: 
0: The price of an iPhone fell 10% in a particular month and another 20% in the second month.  If the initial price was $1000, calculate the price after the second month.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
0: 
0: I'd be happy to help you with that. Here's the solution:
0: 
0: Initially, the price of the iPhone is $1000.
0: 
0: In the first month, the price falls by 10%, which means the new price is:
0: $1000 - (10% of $1000) = $1000 - $100 = $900
0: 
0: In the second month, the price falls by another 20%, which means the new price is:
0: $900 - (20% of $900) = $900 - $180 = $720
0: 
0: Let's verify our calculation by finding the total percentage decrease:
0: Initial price = $1000
0: Price after first month = $900
0: Price after second month = $720
0: 
0: The total percentage decrease = (Initial price - Price after second month) / Initial price
0: = ($1000 - $720) / $1000
0: = $280 / $1000
0: = 28%
0: 
0: This matches the expected total percentage decrease of 30% (10% in the first month + 20% in the second month).
0: 
0:  The answer is $720.<|eot_id|>
0: label_ids:
0: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 4265, 387, 6380, 311, 1520, 499, 449, 430, 13, 5810, 596, 279, 6425, 1473, 91084, 11, 279, 3430, 315, 279, 12443, 374, 400, 1041, 15, 382, 644, 279, 1176, 2305, 11, 279, 3430, 17503, 555, 220, 605, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 1041, 15, 482, 320, 605, 4, 315, 400, 1041, 15, 8, 284, 400, 1041, 15, 482, 400, 1041, 284, 400, 7467, 271, 644, 279, 2132, 2305, 11, 279, 3430, 17503, 555, 2500, 220, 508, 13689, 902, 3445, 279, 502, 3430, 374, 512, 3, 7467, 482, 320, 508, 4, 315, 400, 7467, 8, 284, 400, 7467, 482, 400, 5245, 284, 400, 13104, 271, 10267, 596, 10356, 1057, 22702, 555, 9455, 279, 2860, 11668, 18979, 512, 6475, 3430, 284, 400, 1041, 15, 198, 7117, 1306, 1176, 2305, 284, 400, 7467, 1
0: 98, 7117, 1306, 2132, 2305, 284, 400, 13104, 271, 791, 2860, 11668, 18979, 284, 320, 6475, 3430, 482, 8650, 1306, 2132, 2305, 8, 611, 4220, 3430, 198, 28, 1746, 1041, 15, 482, 400, 13104, 8, 611, 400, 1041, 15, 198, 28, 400, 11209, 611, 400, 1041, 15, 198, 28, 220, 1591, 15804, 2028, 9248, 279, 3685, 2860, 11668, 18979, 315, 220, 966, 4, 320, 605, 4, 304, 279, 1176, 2305, 489, 220, 508, 4, 304, 279, 2132, 2305, 3677, 578, 4320, 374, 400, 13104, 13, 128009]
0: labels:
0: I'd be happy to help you with that. Here's the solution:
0: 
0: Initially, the price of the iPhone is $1000.
0: 
0: In the first month, the price falls by 10%, which means the new price is:
0: $1000 - (10% of $1000) = $1000 - $100 = $900
0: 
0: In the second month, the price falls by another 20%, which means the new price is:
0: $900 - (20% of $900) = $900 - $180 = $720
0: 
0: Let's verify our calculation by finding the total percentage decrease:
0: Initial price = $1000
0: Price after first month = $900
0: Price after second month = $720
0: 
0: The total percentage decrease = (Initial price - Price after second month) / Initial price
0: = ($1000 - $720) / $1000
0: = $280 / $1000
0: = 28%
0: 
0: This matches the expected total percentage decrease of 30% (10% in the first month + 20% in the second month).
0: 
0:  The answer is $720.<|eot_id|>
0: [INFO|configuration_utils.py:679] 2025-01-02 11:50:19,045 >> loading configuration file config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
0: [INFO|configuration_utils.py:746] 2025-01-02 11:50:19,046 >> Model config LlamaConfig {
0:   "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
0:   "architectures": [
0:     "LlamaForCausalLM"
0:   ],
0:   "attention_bias": false,
0:   "attention_dropout": 0.0,
0:   "bos_token_id": 128000,
0:   "eos_token_id": 128009,
0:   "head_dim": 128,
0:   "hidden_act": "silu",
0:   "hidden_size": 4096,
0:   "initializer_range": 0.02,
0:   "intermediate_size": 14336,
0:   "max_position_embeddings": 8192,
0:   "mlp_bias": false,
0:   "model_type": "llama",
0:   "num_attention_heads": 32,
0:   "num_hidden_layers": 32,
0:   "num_key_value_heads": 8,
0:   "pretraining_tp": 1,
0:   "rms_norm_eps": 1e-05,
0:   "rope_scaling": null,
0:   "rope_theta": 500000.0,
0:   "tie_word_embeddings": false,
0:   "torch_dtype": "bfloat16",
0:   "transformers_version": "4.46.1",
0:   "use_cache": true,
0:   "vocab_size": 128256
0: }
0: 
0: [INFO|modeling_utils.py:3937] 2025-01-02 11:50:19,374 >> loading weights file model.safetensors from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/model.safetensors.index.json
0: [2025-01-02 11:50:19,383] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
0: [2025-01-02 11:50:19,384] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
0: [INFO|modeling_utils.py:4080] 2025-01-02 11:50:19,384 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
0: [2025-01-02 11:50:19,384] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
0: [2025-01-02 11:50:19,384] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
0: [WARNING|logging.py:168] 2025-01-02 11:50:19,386 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
0: [WARNING|logging.py:168] 2025-01-02 11:50:19,386 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0: [INFO|configuration_utils.py:1096] 2025-01-02 11:50:19,390 >> Generate config GenerationConfig {
0:   "bos_token_id": 128000,
0:   "eos_token_id": 128009
0: }
0: 
0: [2025-01-02 11:50:20,016] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
0: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:00,  4.17it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:00,  4.21it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:00,  3.11it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:02,  1.05s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:02,  1.07s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:02,  1.06s/it]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:03<00:11,  3.73s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:02,  2.19s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:02,  2.20s/it]Loading checkpoint s
0: hards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.39s/it]
0: Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:05<00:02,  2.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.42s/it]
0: Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.45s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.44s/it]
0: Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:07<00:07,  3.75s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:10<00:03,  3.51s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:11<00:00,  2.47s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:11<00:00,  2.90s/it]
0: [INFO|modeling_utils.py:4800] 2025-01-02 11:50:31,648 >> All model checkpoint weights were used when initializing LlamaForCausalLM.
0: 
0: [INFO|modeling_utils.py:4808] 2025-01-02 11:50:31,648 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.
0: If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
0: [INFO|configuration_utils.py:1051] 2025-01-02 11:50:31,702 >> loading configuration file generation_config.json from cache at /work/nvme/bcrc/cjin1/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/generation_config.json
0: [INFO|configuration_utils.py:1096] 2025-01-02 11:50:31,703 >> Generate config GenerationConfig {
0:   "bos_token_id": 128000,
0:   "do_sample": true,
0:   "eos_token_id": [
0:     128001,
0:     128009
0:   ],
0:   "max_length": 4096,
0:   "temperature": 0.6,
0:   "top_p": 0.9
0: }
0: 
0: [INFO|2025-01-02 11:50:31] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
0: [INFO|2025-01-02 11:50:31] llamafactory.model.model_utils.attention:157 >> Using FlashAttention-2 for faster training and inference.
0: [INFO|2025-01-02 11:50:31] llamafactory.model.adapter:157 >> ZeRO3 / FSDP detected, remaining trainable params in float32.
0: [INFO|2025-01-02 11:50:31] llamafactory.model.adapter:157 >> Fine-tuning method: Full
0: [INFO|2025-01-02 11:50:31] llamafactory.model.loader:157 >> trainable params: 8,030,261,248 || all params: 8,030,261,248 || trainable%: 100.0000
0: [INFO|trainer.py:698] 2025-01-02 11:50:31,728 >> Using auto half precision backend
0: [2025-01-02 11:50:31,902] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
0: [2025-01-02 11:50:31,903] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
0: [2025-01-02 11:50:31,912] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
0: [2025-01-02 11:50:31,913] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
0: [2025-01-02 11:50:31,913] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
0: [2025-01-02 11:50:31,921] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
0: [2025-01-02 11:50:31,921] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
0: [2025-01-02 11:50:31,921] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
0: [2025-01-02 11:50:31,921] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
0: [2025-01-02 11:50:32,081] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
0: [2025-01-02 11:50:32,082] [INFO] [utils.py:782:see_memory_usage] MA 3.74 GB         Max_MA 6.43 GB         CA 5.77 GB         Max_CA 7 GB 
0: [2025-01-02 11:50:32,082] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 127.9 GB, percent = 14.9%
0: [2025-01-02 11:50:32,084] [INFO] [stage3.py:166:__init__] Reduce bucket size 16777216
0: [2025-01-02 11:50:32,084] [INFO] [stage3.py:167:__init__] Prefetch bucket size 15099494
0: [2025-01-02 11:50:32,243] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
0: [2025-01-02 11:50:32,243] [INFO] [utils.py:782:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.77 GB         Max_CA 6 GB 
0: [2025-01-02 11:50:32,244] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 127.9 GB, percent = 14.9%
0: Parameter Offload: Total persistent parameters: 266240 in 65 params
0: [2025-01-02 11:50:32,422] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
0: [2025-01-02 11:50:32,423] [INFO] [utils.py:782:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.77 GB         Max_CA 6 GB 
0: [2025-01-02 11:50:32,423] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 127.9 GB, percent = 14.9%
0: [2025-01-02 11:50:32,583] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
0: [2025-01-02 11:50:32,584] [INFO] [utils.py:782:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 5.77 GB         Max_CA 6 GB 
0: [2025-01-02 11:50:32,584] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 127.9 GB, percent = 14.9%
0: [2025-01-02 11:50:45,871] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 3
0: [2025-01-02 11:50:45,872] [INFO] [utils.py:782:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 6 GB 
0: [2025-01-02 11:50:45,873] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 147.88 GB, percent = 17.3%
0: [2025-01-02 11:50:46,041] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
0: [2025-01-02 11:50:46,041] [INFO] [utils.py:782:see_memory_usage] MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 4 GB 
0: [2025-01-02 11:50:46,042] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 147.88 GB, percent = 17.3%
0: [2025-01-02 11:50:46,233] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
0: [2025-01-02 11:50:46,234] [INFO] [utils.py:782:see_memory_usage] MA 11.22 GB         Max_MA 13.09 GB         CA 13.09 GB         Max_CA 13 GB 
0: [2025-01-02 11:50:46,234] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 157.25 GB, percent = 18.4%
0: [2025-01-02 11:50:46,403] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
0: [2025-01-02 11:50:46,404] [INFO] [utils.py:782:see_memory_usage] MA 11.22 GB         Max_MA 11.22 GB         CA 13.09 GB         Max_CA 13 GB 
0: [2025-01-02 11:50:46,404] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 168.45 GB, percent = 19.7%
0: [2025-01-02 11:50:46,582] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
0: [2025-01-02 11:50:46,583] [INFO] [utils.py:782:see_memory_usage] MA 11.22 GB         Max_MA 14.96 GB         CA 16.83 GB         Max_CA 17 GB 
0: [2025-01-02 11:50:46,583] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 172.19 GB, percent = 20.1%
0: [2025-01-02 11:50:46,584] [INFO] [stage3.py:521:_setup_for_real_optimizer] optimizer state initialized
0: [2025-01-02 11:50:46,887] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
0: [2025-01-02 11:50:46,888] [INFO] [utils.py:782:see_memory_usage] MA 14.99 GB         Max_MA 16.95 GB         CA 17.81 GB         Max_CA 18 GB 
0: [2025-01-02 11:50:46,888] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 176.21 GB, percent = 20.6%
0: [2025-01-02 11:50:46,889] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
0: [2025-01-02 11:50:46,889] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
0: [2025-01-02 11:50:46,889] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
0: [2025-01-02 11:50:46,889] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
0: [2025-01-02 11:50:46,890] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
0: [2025-01-02 11:50:46,890] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
0:     "partition_activations": false, 
0:     "contiguous_memory_optimization": false, 
0:     "cpu_checkpointing": false, 
0:     "number_checkpoints": null, 
0:     "synchronize_checkpoint_boundary": false, 
0:     "profile": false
0: }
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   amp_enabled .................. False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   amp_params ................... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   autotuning_config ............ {
0:     "enabled": false, 
0:     "start_step": null, 
0:     "end_step": null, 
0:     "metric_path": null, 
0:     "arg_mappings": null, 
0:     "metric": "throughput", 
0:     "model_info": null, 
0:     "results_dir": "autotuning_results", 
0:     "exps_dir": "autotuning_exps", 
0:     "overwrite": true, 
0:     "fast": true, 
0:     "start_profile_step": 3, 
0:     "end_profile_step": 5, 
0:     "tuner_type": "gridsearch", 
0:     "tuner_early_stopping": 5, 
0:     "tuner_num_trials": 50, 
0:     "model_info_path": null, 
0:     "mp_size": 1, 
0:     "max_train_batch_size": null, 
0:     "min_train_batch_size": 1, 
0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
0:     "min_train_micro_batch_size_per_gpu": 1, 
0:     "num_tuning_micro_batch_sizes": 3
0: }
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xfffa62333e50>
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   communication_data_type ...... None
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_pa
0: rameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   disable_allgather ............ False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   dump_state ................... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
0: [2025-01-02 11:50:46,891] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
0:     "enabled": false, 
0:     "recompute_fwd_factor": 0.0, 
0:     "profile_step": 1, 
0:     "module_depth": -1, 
0:     "top_modules": 1, 
0:     "detailed": true, 
0:     "output_file": null
0: }
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   fp16_enabled ................. False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   global_rank .................. 0
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 2
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   graph_harvesting ............. False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   memory_breakdown ............. False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   nebula_config ................ {
0:     "enabled": false, 
0:     "persistent_storage_path": null, 
0:     "persistent_time_interval": 100, 
0:     "num_of_version_in_retention": 2, 
0:     "enable_nebula_load": true, 
0:     "load_path": null
0: }
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   optimizer_name ............... None
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   optimizer_params ............. None
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   pld_enabled .................. False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   pld_params ................... False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   prescale_gradients ........... False
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   scheduler_name ............... None
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   scheduler_params ............. None
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
0: [2025-01-02 11:50:46,892] [INFO] [config.py:1003:print]   sparse_attention ............. None
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   steps_per_print .............. inf
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   train_batch_size ............. 64
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   weight_quantization_config ... None
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   world_size ................... 4
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params
0: _gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   zero_enabled ................. True
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
0: [2025-01-02 11:50:46,893] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
0: [2025-01-02 11:50:46,893] [INFO] [config.py:989:print_user_config]   json = {
0:     "train_batch_size": 64, 
0:     "train_micro_batch_size_per_gpu": 8, 
0:     "gradient_accumulation_steps": 2, 
0:     "gradient_clipping": 1.0, 
0:     "zero_allow_untested_optimizer": true, 
0:     "fp16": {
0:         "enabled": false, 
0:         "loss_scale": 0, 
0:         "loss_scale_window": 1000, 
0:         "initial_scale_power": 16, 
0:         "hysteresis": 2, 
0:         "min_loss_scale": 1
0:     }, 
0:     "bf16": {
0:         "enabled": true
0:     }, 
0:     "zero_optimization": {
0:         "stage": 3, 
0:         "overlap_comm": true, 
0:         "contiguous_gradients": true, 
0:         "sub_group_size": 1.000000e+09, 
0:         "reduce_bucket_size": 1.677722e+07, 
0:         "stage3_prefetch_bucket_size": 1.509949e+07, 
0:         "stage3_param_persistence_threshold": 4.096000e+04, 
0:         "stage3_max_live_parameters": 1.000000e+09, 
0:         "stage3_max_reuse_distance": 1.000000e+09, 
0:         "stage3_gather_16bit_weights_on_model_save": true
0:     }, 
0:     "steps_per_print": inf
0: }
0: [INFO|trainer.py:2313] 2025-01-02 11:50:46,893 >> ***** Running training *****
0: [INFO|trainer.py:2314] 2025-01-02 11:50:46,893 >>   Num examples = 7,473
0: [INFO|trainer.py:2315] 2025-01-02 11:50:46,893 >>   Num Epochs = 1
0: [INFO|trainer.py:2316] 2025-01-02 11:50:46,893 >>   Instantaneous batch size per device = 8
0: [INFO|trainer.py:2319] 2025-01-02 11:50:46,893 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
0: [INFO|trainer.py:2320] 2025-01-02 11:50:46,893 >>   Gradient Accumulation steps = 2
0: [INFO|trainer.py:2321] 2025-01-02 11:50:46,893 >>   Total optimization steps = 117
0: [INFO|trainer.py:2322] 2025-01-02 11:50:46,894 >>   Number of trainable parameters = 8,030,261,248
0: [INFO|integration_utils.py:812] 2025-01-02 11:50:46,896 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
0: wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
0: wandb: Currently logged in as: jincan333. Use `wandb login --relogin` to force relogin
0: wandb: Tracking run with wandb version 0.19.1
0: wandb: Run data is saved locally in /u/cjin1/LLaMA-Factory/wandb/run-20250102_115047-yoe535xt
0: wandb: Run `wandb offline` to turn off syncing.
0: wandb: Syncing run llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048
0: wandb: ‚≠êÔ∏è View project at https://wandb.ai/jincan333/safe_reason
0: wandb: üöÄ View run at https://wandb.ai/jincan333/safe_reason/runs/yoe535xt
0: {'loss': 0.6678, 'grad_norm': 12.044972631284017, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.09}
0:   0%|          | 0/117 [00:00<?, ?it/s]  1%|          | 1/117 [00:04<08:33,  4.42s/it]  2%|‚ñè         | 2/117 [00:06<05:17,  2.76s/it]  3%|‚ñé         | 3/117 [00:07<04:10,  2.20s/it]  3%|‚ñé         | 4/117 [00:09<03:39,  1.94s/it]  4%|‚ñç         | 5/117 [00:10<03:34,  1.91s/it]  5%|‚ñå         | 6/117 [00:12<03:29,  1.89s/it]  6%|‚ñå         | 7/117 [00:14<03:15,  1.78s/it]  7%|‚ñã         | 8/117 [00:16<03:10,  1.75s/it]  8%|‚ñä         | 9/117 [00:17<03:06,  1.73s/it]  9%|‚ñä         | 10/117 [00:19<02:59,  1.68s/it]                                                  9%|‚ñä         | 10/117 [00:19<02:59,  1.68s/it]  9%|‚ñâ         | 11/117 [00:20<02:53,  1.64s/it] 10%|‚ñà         | 12/117 [00:22<02:49,  1.62s/it] 11%|‚ñà         | 13/117 [00:23<02:46,  1.60s/it] 12%|‚ñà‚ñè        | 14/117 [00:25<02:44,  1.59s/it] 13%|‚ñà‚ñé        | 15/117 [00:27<02:42,  1.59s/it] 14%|‚ñà‚ñé        | 16/117 [00:29<02:49,  1.68s/it] 15%|‚ñà‚ñç        | 17/117 [00:30<02:45,  1.66s/it] 15%|‚ñà‚ñå       
0: {'loss': 0.5708, 'grad_norm': 5.769965907719363, 'learning_rate': 4.928725095732168e-07, 'epoch': 0.17}
0: {'loss': 0.4476, 'grad_norm': 3.244608500789503, 'learning_rate': 4.646121984004665e-07, 'epoch': 0.26}
0:  | 18/117 [00:32<02:40,  1.62s/it] 16%|‚ñà‚ñå        | 19/117 [00:33<02:37,  1.61s/it] 17%|‚ñà‚ñã        | 20/117 [00:35<02:35,  1.60s/it]                                                 17%|‚ñà‚ñã        | 20/117 [00:35<02:35,  1.60s/it] 18%|‚ñà‚ñä        | 21/117 [00:36<02:33,  1.59s/it] 19%|‚ñà‚ñâ        | 22/117 [00:38<02:30,  1.59s/it] 20%|‚ñà‚ñâ        | 23/117 [00:40<02:31,  1.62s/it] 21%|‚ñà‚ñà        | 24/117 [00:41<02:30,  1.62s/it] 21%|‚ñà‚ñà‚ñè       | 25/117 [00:43<02:29,  1.63s/it] 22%|‚ñà‚ñà‚ñè       | 26/117 [00:45<02:28,  1.63s/it] 23%|‚ñà‚ñà‚ñé       | 27/117 [00:46<02:25,  1.62s/it] 24%|‚ñà‚ñà‚ñç       | 28/117 [00:48<02:22,  1.60s/it] 25%|‚ñà‚ñà‚ñç       | 29/117 [00:49<02:19,  1.59s/it] 26%|‚ñà‚ñà‚ñå       | 30/117 [00:51<02:17,  1.58s/it]                                                 26%|‚ñà‚ñà‚ñå       | 30/117 [00:51<02:17,  1.58s/it] 26%|‚ñà‚ñà‚ñã       | 31/117 [00:53<02:20,  1.63s/it] 27%|‚ñà‚ñà‚ñã       | 32/117 [00:54<02:16,  1.61s/it] 28%|‚ñà‚ñà‚ñä       | 
0: {'loss': 0.39, 'grad_norm': 2.794140679797416, 'learning_rate': 4.172826515897145e-07, 'epoch': 0.34}
0: 33/117 [00:56<02:15,  1.62s/it] 29%|‚ñà‚ñà‚ñâ       | 34/117 [00:57<02:13,  1.60s/it] 30%|‚ñà‚ñà‚ñâ       | 35/117 [00:59<02:12,  1.61s/it] 31%|‚ñà‚ñà‚ñà       | 36/117 [01:01<02:09,  1.60s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 37/117 [01:02<02:12,  1.65s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 38/117 [01:04<02:10,  1.66s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 39/117 [01:06<02:08,  1.65s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 40/117 [01:07<02:07,  1.65s/it]                                                 34%|‚ñà‚ñà‚ñà‚ñç      | 40/117 [01:07<02:07,  1.65s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 41/117 [01:09<02:04,  1.64s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 42/117 [01:10<02:01,  1.63s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 43/117 [01:12<02:01,  1.65s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 44/117 [01:14<01:58,  1.62s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 45/117 [01:15<01:58,  1.64s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 46/117 [01:17<01:57,  1.66s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 47/117 [01:19<02:02,  1.74s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 48/117 [01:21<01:57,  1.70s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 49/117 [0
0: {'loss': 0.3628, 'grad_norm': 2.522163375971969, 'learning_rate': 3.550893070773914e-07, 'epoch': 0.43}
0: {'loss': 0.3587, 'grad_norm': 2.530453835549648, 'learning_rate': 2.8355831645441387e-07, 'epoch': 0.51}
0: 1:22<01:56,  1.72s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50/117 [01:24<01:52,  1.67s/it]                                                 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 50/117 [01:24<01:52,  1.67s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 51/117 [01:26<01:49,  1.66s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 52/117 [01:27<01:46,  1.63s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 53/117 [01:29<01:47,  1.67s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 54/117 [01:31<01:44,  1.65s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 55/117 [01:32<01:40,  1.63s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 56/117 [01:34<01:38,  1.61s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 57/117 [01:35<01:35,  1.60s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 58/117 [01:37<01:37,  1.64s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 59/117 [01:39<01:34,  1.62s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60/117 [01:40<01:31,  1.61s/it]                                                 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 60/117 [01:40<01:31,  1.61s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 61/117 [01:42<01:30,  1.62s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 62/117 [01:43<01:28,  1.60s/it] 54%|‚ñà
0: {'loss': 0.3451, 'grad_norm': 2.450703852993037, 'learning_rate': 2.0904552214621556e-07, 'epoch': 0.6}
0: ‚ñà‚ñà‚ñà‚ñà‚ñç    | 63/117 [01:45<01:26,  1.59s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 64/117 [01:47<01:24,  1.59s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 65/117 [01:48<01:22,  1.59s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 66/117 [01:50<01:20,  1.58s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 67/117 [01:52<01:23,  1.67s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 68/117 [01:53<01:20,  1.65s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 69/117 [01:55<01:18,  1.64s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 70/117 [01:56<01:15,  1.62s/it]                                                 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 70/117 [01:56<01:15,  1.62s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 71/117 [01:58<01:14,  1.61s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 72/117 [02:00<01:12,  1.60s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 73/117 [02:01<01:09,  1.59s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 74/117 [02:03<01:08,  1.58s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 75/117 [02:04<01:06,  1.58s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 76/117 [02:06<01:04,  1.58s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 77/117 [02:07<01:03,  1.58
0: {'loss': 0.3419, 'grad_norm': 2.452850609985889, 'learning_rate': 1.381717129210918e-07, 'epoch': 0.68}
0: {'loss': 0.3383, 'grad_norm': 2.380666098380846, 'learning_rate': 7.723433775328384e-08, 'epoch': 0.77}
0: s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 78/117 [02:09<01:02,  1.59s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 79/117 [02:11<01:04,  1.69s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 80/117 [02:13<01:02,  1.69s/it]                                                 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 80/117 [02:13<01:02,  1.69s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 81/117 [02:14<00:59,  1.66s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 82/117 [02:16<00:57,  1.65s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 83/117 [02:18<00:56,  1.66s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 84/117 [02:19<00:53,  1.64s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 85/117 [02:21<00:51,  1.61s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 86/117 [02:22<00:50,  1.64s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 87/117 [02:24<00:49,  1.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 88/117 [02:26<00:47,  1.65s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 89/117 [02:27<00:47,  1.68s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 90/117 [02:29<00:44,  1.65s/it]                                                 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñ
0: {'loss': 0.3433, 'grad_norm': 2.3201738723383847, 'learning_rate': 3.1647949845717585e-08, 'epoch': 0.85}
0: à‚ñà‚ñã  | 90/117 [02:29<00:44,  1.65s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 91/117 [02:31<00:42,  1.63s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 92/117 [02:32<00:40,  1.63s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 93/117 [02:34<00:38,  1.60s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 94/117 [02:35<00:37,  1.62s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 95/117 [02:37<00:35,  1.60s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 96/117 [02:39<00:33,  1.59s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 97/117 [02:40<00:31,  1.59s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 98/117 [02:42<00:30,  1.59s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 99/117 [02:43<00:28,  1.60s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 100/117 [02:45<00:27,  1.59s/it]                                                  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 100/117 [02:45<00:27,  1.59s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 101/117 [02:47<00:25,  1.62s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 102/117 [02:48<00:24,  1.61s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 103/117 [02:50<00:23,  1.64s/i
0: {'loss': 0.3272, 'grad_norm': 2.3167451729814474, 'learning_rate': 5.463099816548577e-09, 'epoch': 0.94}
0: t] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 104/117 [02:52<00:21,  1.64s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 105/117 [02:53<00:19,  1.62s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 106/117 [02:55<00:17,  1.62s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 107/117 [02:56<00:16,  1.63s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 108/117 [02:58<00:14,  1.62s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 109/117 [03:00<00:12,  1.60s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 110/117 [03:01<00:11,  1.59s/it]                                                  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 110/117 [03:01<00:11,  1.59s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 111/117 [03:03<00:09,  1.59s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 112/117 [03:04<00:07,  1.60s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 113/117 [03:06<00:06,  1.59s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 114/117 [03:08<00:04,  1.62s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 115/117 [03:10<00:03,  1.76s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 116/117 [03:11<00:01,  1.73s/
0: it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117/117 [03:13<00:00,  1.69s/it][INFO|trainer.py:3801] 2025-01-02 11:55:03,903 >> Saving model checkpoint to /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117
0: [INFO|configuration_utils.py:414] 2025-01-02 11:55:03,910 >> Configuration saved in /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/config.json
0: [INFO|configuration_utils.py:865] 2025-01-02 11:55:03,913 >> Configuration saved in /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/generation_config.json
0: [INFO|modeling_utils.py:3043] 2025-01-02 11:55:11,175 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/model.safetensors.index.json.
0: [INFO|tokenization_utils_base.py:2646] 2025-01-02 11:55:11,180 >> tokenizer config file saved in /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/tokenizer_config.json
0: [INFO|tokenization_utils_base.py:2655] 2025-01-02 11:55:11,182 >> Special tokens file saved in /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/special_tokens_map.json
0: [2025-01-02 11:55:11,400] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step117 is about to be saved!
0: [2025-01-02 11:55:11,411] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/global_step117/zero_pp_rank_0_mp_rank_00_model_states.pt
0: [2025-01-02 11:55:11,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/global_step117/zero_pp_rank_0_mp_rank_00_model_states.pt...
0: [2025-01-02 11:55:11,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/global_step117/zero_pp_rank_0_mp_rank_00_model_states.pt.
0: [2025-01-02 11:55:11,428] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-7_cl2048/checkpoint-117/global_step117/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
