0: num_nodes: 1
0: SLURM_NODEID: 0
0: CUDA_VISIBLE_DEVICES: 0,1,2,3
0: output_dir: /work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048
0: The following values were not passed to `accelerate launch` and had defaults used instead:
0: 	`--num_processes` was set to a value of `1`
0: 	`--num_machines` was set to a value of `1`
0: 	`--mixed_precision` was set to a value of `'no'`
0: 	`--dynamo_backend` was set to a value of `'no'`
0: To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
0: wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
0: wandb: Currently logged in as: jincan333. Use `wandb login --relogin` to force relogin
0: wandb: Tracking run with wandb version 0.19.1
0: wandb: Run data is saved locally in /u/cjin1/LLaMA-Factory/wandb/run-20250102_015040-6be4badz
0: wandb: Run `wandb offline` to turn off syncing.
0: wandb: Syncing run llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048_eval
0: wandb: ‚≠êÔ∏è View project at https://wandb.ai/jincan333/safe_reason
0: wandb: üöÄ View run at https://wandb.ai/jincan333/safe_reason/runs/6be4badz
0: 2025-01-02:01:50:41,258 INFO     [__main__.py:279] Verbosity set to INFO
0: 2025-01-02:01:50:51,008 INFO     [__main__.py:376] Selected Tasks: ['gsm8k']
0: 2025-01-02:01:50:51,021 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
0: 2025-01-02:01:50:51,022 WARNING  [evaluator.py:175] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!
0: 2025-01-02:01:50:51,022 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': '/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048', 'dtype': 'auto'}
0: 2025-01-02:01:50:52,681 INFO     [huggingface.py:132] Using device 'cuda'
0: Traceback (most recent call last):
0:   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
0:     return _run_code(code, main_globals, None,
0:   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
0:     exec(code, run_globals)
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/__main__.py", line 461, in <module>
0:     cli_evaluate()
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/__main__.py", line 382, in cli_evaluate
0:     results = evaluator.simple_evaluate(
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/utils.py", line 402, in _wrapper
0:     return fn(*args, **kwargs)
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/evaluator.py", line 204, in simple_evaluate
0:     lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/api/model.py", line 147, in create_from_arg_string
0:     return cls(**args, **args2)
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/models/huggingface.py", line 175, in __init__
0:     self._create_tokenizer(
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/models/huggingface.py", line 707, in _create_tokenizer
0:     self.tokenizer = transformers.AutoTokenizer.from_pretrained(
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 939, in from_pretrained
0:     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2197, in from_pretrained
0:     raise EnvironmentError(
0: OSError: Can't load tokenizer for '/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.
0: Traceback (most recent call last):
0:   File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
0:     return _run_code(code, main_globals, None,
0:   File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
0:     exec(code, run_globals)
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/__main__.py", line 461, in <module>
0:     cli_evaluate()
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/__main__.py", line 382, in cli_evaluate
0:     results = evaluator.simple_evaluate(
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/utils.py", line 402, in _wrapper
0:     return fn(*args, **kwargs)
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/evaluator.py", line 204, in simple_evaluate
0:     lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/api/model.py", line 147, in create_from_arg_string
0:     return cls(**args, **args2)
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/models/huggingface.py", line 175, in __init__
0:     self._create_tokenizer(
0:   File "/u/cjin1/lm-evaluation-harness/lm_eval/models/huggingface.py", line 707, in _create_tokenizer
0:     self.tokenizer = transformers.AutoTokenizer.from_pretrained(
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 939, in from_pretrained
0:     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2197, in from_pretrained
0:     raise EnvironmentError(
0: OSError: Can't load tokenizer for '/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.
0: [1;34mwandb[0m: üöÄ View run [33mllama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048_eval[0m at: [34mhttps://wandb.ai/jincan333/safe_reason/runs/6be4badz[0m
0: [1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250102_015040-6be4badz/logs[0m
0: Traceback (most recent call last):
0:   File "/u/cjin1/.my_envs/llamafactory/bin/accelerate", line 8, in <module>
0:     sys.exit(main())
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
0:     args.func(args)
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1168, in launch_command
0:     simple_launcher(args)
0:   File "/u/cjin1/.my_envs/llamafactory/lib/python3.10/site-packages/accelerate/commands/launch.py", line 763, in simple_launcher
0:     raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
0: subprocess.CalledProcessError: Command '['/usr/bin/python', '-m', 'lm_eval', '--model', 'hf', '--model_args', 'pretrained=/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048,dtype=auto', '--tasks', 'gsm8k', '--num_fewshot', '5', '--gen_kwargs', 'top_p=1,temperature=0', '--output_path', '/work/nvme/bcrc/cjin1/safe_reason/llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048/gsm8k', '--log_samples', '--write_out', '--apply_chat_template', '--wandb_args', 'project=safe_reason,name=llama3_sft_gsm8k_rectify_sft_tbs8_ebs8_lr5e-6_cl2048_eval', '--batch_size', '128']' returned non-zero exit status 1.
srun: error: gh030: task 0: Exited with exit code 1
