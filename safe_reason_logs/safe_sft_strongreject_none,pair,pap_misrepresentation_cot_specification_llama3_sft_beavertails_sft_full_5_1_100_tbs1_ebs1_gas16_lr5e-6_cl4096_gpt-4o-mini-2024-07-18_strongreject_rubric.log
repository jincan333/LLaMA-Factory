{
    "model": "/research/cbim/medical/cj574/safe_reason/llama3_sft_beavertails_sft_full_5_1_100_tbs1_ebs1_gas16_lr5e-6_cl4096",
    "judge_model": "gpt-4o-mini-2024-07-18",
    "dataset": "strongreject",
    "jailbreak": "none,pair,pap_misrepresentation",
    "cot_prompt": "cot_specification",
    "evaluator": "strongreject_rubric",
    "temperature": 0.0,
    "top_p": 1.0,
    "max_length": 4096,
    "generate": true,
    "evaluate": true
}
loading dataset strongreject and applying jailbreak none,pair,pap_misrepresentation
dict_keys(['auto_payload_splitting', 'auto_obfuscation', 'rot_13', 'disemvowel', 'pair', 'pap_evidence_based_persuasion', 'pap_expert_endorsement', 'pap_misrepresentation', 'pap_authority_endorsement', 'pap_logical_appeal', 'translation_hmong', 'translation_scotts_gaelic', 'translation_guarani', 'translation_zulu', 'gcg_transfer_harmbench', 'gcg_transfer_universal_attacks', 'combination_3', 'combination_2', 'few_shot_json', 'dev_mode_v2', 'dev_mode_with_rant', 'wikipedia_with_title', 'distractors', 'wikipedia', 'style_injection_json', 'style_injection_short', 'refusal_suppression', 'prefix_injection', 'distractors_negated', 'poems', 'base64', 'base64_raw', 'base64_input_only', 'base64_output_only', 'none', 'evil_confidant', 'aim'])
generating responses for /research/cbim/medical/cj574/safe_reason/llama3_sft_beavertails_sft_full_5_1_100_tbs1_ebs1_gas16_lr5e-6_cl4096 and none,pair,pap_misrepresentation
Map:   0%|          | 0/939 [00:00<?, ? examples/s]Map: 100%|██████████| 939/939 [00:00<00:00, 1809.21 examples/s]Map: 100%|██████████| 939/939 [00:00<00:00, 1798.33 examples/s]
Map (num_proc=32):   0%|          | 0/939 [00:00<?, ? examples/s]