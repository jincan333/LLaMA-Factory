{
    "model": "/research/cbim/medical/cj574/safe_reason/llama3_sft_dr_sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_5_1_100_1000_0_tbs8_ebs8_gas2_lr5e-6_cl4096",
    "judge_model": "gpt-4o-mini-2024-07-18",
    "dataset": "xstest",
    "jailbreak": "none",
    "cot_prompt": "none",
    "evaluator": "strongreject_rubric",
    "temperature": 0.0,
    "top_p": 1.0,
    "max_length": 4096,
    "generate": true,
    "evaluate": true,
    "gpu": "cuda:0"
}
loading dataset xstest and applying jailbreak none
dict_keys(['auto_payload_splitting', 'auto_obfuscation', 'rot_13', 'disemvowel', 'pair', 'pap_evidence_based_persuasion', 'pap_expert_endorsement', 'pap_misrepresentation', 'pap_authority_endorsement', 'pap_logical_appeal', 'translation_hmong', 'translation_scotts_gaelic', 'translation_guarani', 'translation_zulu', 'gcg_transfer_harmbench', 'gcg_transfer_universal_attacks', 'combination_3', 'combination_2', 'few_shot_json', 'dev_mode_v2', 'dev_mode_with_rant', 'wikipedia_with_title', 'distractors', 'wikipedia', 'style_injection_json', 'style_injection_short', 'refusal_suppression', 'prefix_injection', 'distractors_negated', 'poems', 'base64', 'base64_raw', 'base64_input_only', 'base64_output_only', 'none', 'evil_confidant', 'aim'])
INFO 01-25 14:52:50 config.py:510] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.
INFO 01-25 14:52:50 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='/research/cbim/medical/cj574/safe_reason/llama3_sft_dr_sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_5_1_100_1000_0_tbs8_ebs8_gas2_lr5e-6_cl4096', speculative_config=None, tokenizer='/research/cbim/medical/cj574/safe_reason/llama3_sft_dr_sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_5_1_100_1000_0_tbs8_ebs8_gas2_lr5e-6_cl4096', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/research/cbim/medical/cj574/safe_reason/llama3_sft_dr_sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_5_1_100_1000_0_tbs8_ebs8_gas2_lr5e-6_cl4096, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":64}, use_cached_outputs=False, 
INFO 01-25 14:52:52 selector.py:120] Using Flash Attention backend.
INFO 01-25 14:52:52 model_runner.py:1094] Starting to load model /research/cbim/medical/cj574/safe_reason/llama3_sft_dr_sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_5_1_100_1000_0_tbs8_ebs8_gas2_lr5e-6_cl4096...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.83it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.55it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.96it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.80it/s]

INFO 01-25 14:52:55 model_runner.py:1099] Loading model weights took 14.9595 GB
INFO 01-25 14:52:56 worker.py:241] Memory profiling takes 0.91 seconds
INFO 01-25 14:52:56 worker.py:241] the current vLLM instance can use total_gpu_memory (44.45GiB) x gpu_memory_utilization (0.90) = 40.00GiB
INFO 01-25 14:52:56 worker.py:241] model weights take 14.96GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.43GiB; the rest of the memory reserved for KV Cache is 24.55GiB.
INFO 01-25 14:52:56 gpu_executor.py:76] # GPU blocks: 12567, # CPU blocks: 2048
INFO 01-25 14:52:56 gpu_executor.py:80] Maximum concurrency for 4096 tokens per request: 49.09x
INFO 01-25 14:52:58 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/11 [00:00<?, ?it/s]Capturing CUDA graph shapes:   9%|▉         | 1/11 [00:00<00:03,  2.61it/s]Capturing CUDA graph shapes:  18%|█▊        | 2/11 [00:00<00:03,  2.67it/s]Capturing CUDA graph shapes:  27%|██▋       | 3/11 [00:01<00:03,  2.63it/s]Capturing CUDA graph shapes:  36%|███▋      | 4/11 [00:01<00:02,  2.69it/s]Capturing CUDA graph shapes:  45%|████▌     | 5/11 [00:01<00:02,  2.68it/s]Capturing CUDA graph shapes:  55%|█████▍    | 6/11 [00:02<00:01,  2.71it/s]Capturing CUDA graph shapes:  64%|██████▎   | 7/11 [00:02<00:01,  2.75it/s]Capturing CUDA graph shapes:  73%|███████▎  | 8/11 [00:02<00:01,  2.77it/s]Capturing CUDA graph shapes:  82%|████████▏ | 9/11 [00:03<00:00,  2.79it/s]Capturing CUDA graph shapes:  91%|█████████ | 10/11 [00:03<00:00,  2.80it/s]Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:04<00:00,  2.81it/s]Capturing CUDA graph shapes: 100%|██████████| 11/11 [00:04<00:00,  2.75it/s]
INFO 01-25 14:53:02 model_runner.py:1535] Graph capturing finished in 4 secs, took 0.12 GiB
INFO 01-25 14:53:02 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 7.70 seconds
generating responses for /research/cbim/medical/cj574/safe_reason/llama3_sft_dr_sft_preliminary_meta-llama-Meta-Llama-3-8B-Instruct_5_1_100_1000_0_tbs8_ebs8_gas2_lr5e-6_cl4096 and none
Map:   0%|          | 0/250 [00:00<?, ? examples/s]Map: 100%|██████████| 250/250 [00:00<00:00, 7454.53 examples/s]
Processed prompts:   0%|          | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/250 [00:00<02:37,  1.59it/s, est. speed input: 30.12 toks/s, output: 20.61 toks/s]Processed prompts:  25%|██▌       | 63/250 [00:01<00:03, 58.57it/s, est. speed input: 1082.84 toks/s, output: 650.17 toks/s]Processed prompts:  50%|████▉     | 124/250 [00:01<00:01, 76.09it/s, est. speed input: 1402.39 toks/s, output: 853.39 toks/s]Processed prompts:  74%|███████▍  | 185/250 [00:02<00:00, 85.46it/s, est. speed input: 1592.42 toks/s, output: 963.22 toks/s]Processed prompts:  98%|█████████▊| 246/250 [00:02<00:00, 102.70it/s, est. speed input: 1803.50 toks/s, output: 1100.47 toks/s]Processed prompts:  99%|█████████▉| 248/250 [00:15<00:00, 102.70it/s, est. speed input: 342.75 toks/s, output: 240.82 toks/s]  Processed prompts: 100%|█████████▉| 249/250 [00:15<00:00,  7.98it/s, est. speed input: 334.74 toks/s, output: 266.66 toks/s] Processed prompts: 100%|██████████| 250/250 [00:17<00:00,  6.68it/s, est. speed input: 297.48 toks/s, output: 268.23 toks/s]Processed prompts: 100%|██████████| 250/250 [00:17<00:00, 13.98it/s, est. speed input: 297.48 toks/s, output: 268.23 toks/s]
Map:   0%|          | 0/250 [00:00<?, ? examples/s]Map: 100%|██████████| 250/250 [00:00<00:00, 11232.26 examples/s]
Map:   0%|          | 0/250 [00:00<?, ? examples/s]Map: 100%|██████████| 250/250 [00:00<00:00, 10646.41 examples/s]
evaluating strongreject_rubric for gpt-4o-mini-2024-07-18
dict_keys(['string_matching', 'openai_moderation_api', 'pair', 'gpt4_judge', 'jailbroken_binary', 'category_binary', 'strongreject_rubric', 'strongreject_finetuned', 'strongreject_aisi', 'harmbench'])
Map (num_proc=32):   0%|          | 0/250 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 1/250 [00:01<08:02,  1.94s/ examples]Map (num_proc=32):   1%|          | 2/250 [00:02<03:46,  1.09 examples/s]Map (num_proc=32):   1%|          | 3/250 [00:02<02:24,  1.71 examples/s]Map (num_proc=32):   3%|▎         | 8/250 [00:02<00:37,  6.44 examples/s]Map (num_proc=32):   5%|▌         | 13/250 [00:02<00:20, 11.62 examples/s]Map (num_proc=32):   6%|▋         | 16/250 [00:02<00:16, 13.77 examples/s]Map (num_proc=32):   8%|▊         | 19/250 [00:02<00:14, 15.76 examples/s]Map (num_proc=32):  10%|▉         | 24/250 [00:03<00:14, 15.68 examples/s]Map (num_proc=32):  11%|█         | 27/250 [00:03<00:21, 10.31 examples/s]Map (num_proc=32):  12%|█▏        | 29/250 [00:03<00:21, 10.27 examples/s]Map (num_proc=32):  12%|█▏        | 31/250 [00:04<00:32,  6.75 examples/s]Map (num_proc=32):  13%|█▎        | 33/250 [00:04<00:27,  7.90 examples/s]Map (num_proc=32):  14%|█▍        | 36/250 [00:04<00:20, 10.46 examples/s]Map (num_proc=32):  15%|█▌        | 38/250 [00:05<00:22,  9.23 examples/s]Map (num_proc=32):  18%|█▊        | 45/250 [00:05<00:12, 16.23 examples/s]Map (num_proc=32):  19%|█▉        | 48/250 [00:05<00:15, 12.82 examples/s]Map (num_proc=32):  20%|██        | 50/250 [00:05<00:14, 13.43 examples/s]Map (num_proc=32):  21%|██        | 53/250 [00:05<00:12, 15.96 examples/s]Map (num_proc=32):  22%|██▏       | 56/250 [00:06<00:14, 13.84 examples/s]Map (num_proc=32):  24%|██▎       | 59/250 [00:06<00:14, 13.61 examples/s]Map (num_proc=32):  24%|██▍       | 61/250 [00:06<00:18, 10.36 examples/s]Map (num_proc=32):  25%|██▌       | 63/250 [00:07<00:21,  8.53 examples/s]Map (num_proc=32):  27%|██▋       | 67/250 [00:07<00:17, 10.52 examples/s]Map (num_proc=32):  28%|██▊       | 69/250 [00:07<00:16, 10.71 examples/s]Map (num_proc=32):  29%|██▉       | 72/250 [00:07<00:13, 13.45 examples/s]Map (num_proc=32):  30%|██▉       | 74/250 [00:07<00:15, 11.19 examples/s]Map (num_proc=32):  30%|███       | 76/250 [00:08<00:17, 10.10 examples/s]Map (num_proc=32):  32%|███▏      | 79/250 [00:08<00:17,  9.59 examples/s]Map (num_proc=32):  32%|███▏      | 81/250 [00:08<00:15, 10.75 examples/s]Map (num_proc=32):  33%|███▎      | 83/250 [00:08<00:15, 11.01 examples/s]Map (num_proc=32):  34%|███▍      | 85/250 [00:08<00:14, 11.31 examples/s]Map (num_proc=32):  35%|███▍      | 87/250 [00:09<00:14, 11.12 examples/s]Map (num_proc=32):  36%|███▌      | 89/250 [00:09<00:13, 12.21 examples/s]Map (num_proc=32):  37%|███▋      | 92/250 [00:09<00:10, 15.40 examples/s]Map (num_proc=32):  38%|███▊      | 94/250 [00:09<00:11, 13.30 examples/s]Map (num_proc=32):  38%|███▊      | 96/250 [00:09<00:10, 14.28 examples/s]Map (num_proc=32):  39%|███▉      | 98/250 [00:09<00:14, 10.45 examples/s]Map (num_proc=32):  40%|████      | 100/250 [00:10<00:15,  9.45 examples/s]Map (num_proc=32):  41%|████      | 102/250 [00:10<00:21,  6.91 examples/s]Map (num_proc=32):  41%|████      | 103/250 [00:10<00:20,  7.21 examples/s]Map (num_proc=32):  43%|████▎     | 107/250 [00:10<00:11, 11.96 examples/s]Map (num_proc=32):  44%|████▎     | 109/250 [00:11<00:11, 12.39 examples/s]Map (num_proc=32):  44%|████▍     | 111/250 [00:11<00:12, 11.37 examples/s]Map (num_proc=32):  46%|████▌     | 115/250 [00:11<00:11, 11.37 examples/s]Map (num_proc=32):  47%|████▋     | 117/250 [00:11<00:12, 10.54 examples/s]Map (num_proc=32):  48%|████▊     | 119/250 [00:12<00:11, 10.98 examples/s]Map (num_proc=32):  49%|████▉     | 123/250 [00:12<00:08, 14.35 examples/s]Map (num_proc=32):  50%|█████     | 125/250 [00:12<00:11, 10.53 examples/s]Map (num_proc=32):  51%|█████     | 127/250 [00:12<00:11, 10.95 examples/s]Map (num_proc=32):  52%|█████▏    | 129/250 [00:13<00:12,  9.70 examples/s]Map (num_proc=32):  53%|█████▎    | 132/250 [00:13<00:09, 12.61 examples/s]Map (num_proc=32):  54%|█████▍    | 135/250 [00:13<00:09, 12.71 examples/s]Map (num_proc=32):  55%|█████▍    | 137/250 [00:13<00:09, 11.36 examples/s]Map (num_proc=32):  56%|█████▋    | 141/250 [00:13<00:07, 15.12 examples/s]Map (num_proc=32):  57%|█████▋    | 143/250 [00:13<00:08, 13.06 examples/s]Map (num_proc=32):  58%|█████▊    | 145/250 [00:14<00:07, 13.41 examples/s]Map (num_proc=32):  59%|█████▉    | 147/250 [00:14<00:07, 13.67 examples/s]Map (num_proc=32):  60%|█████▉    | 149/250 [00:14<00:08, 11.61 examples/s]Map (num_proc=32):  61%|██████    | 152/250 [00:14<00:07, 12.59 examples/s]Map (num_proc=32):  62%|██████▏   | 154/250 [00:15<00:12,  7.44 examples/s]Map (num_proc=32):  62%|██████▏   | 156/250 [00:15<00:10,  8.60 examples/s]Map (num_proc=32):  63%|██████▎   | 158/250 [00:15<00:10,  8.91 examples/s]Map (num_proc=32):  64%|██████▍   | 160/250 [00:15<00:09,  9.37 examples/s]Map (num_proc=32):  65%|██████▍   | 162/250 [00:15<00:09,  9.74 examples/s]Map (num_proc=32):  66%|██████▌   | 164/250 [00:16<00:07, 11.32 examples/s]Map (num_proc=32):  67%|██████▋   | 167/250 [00:16<00:05, 14.58 examples/s]Map (num_proc=32):  68%|██████▊   | 169/250 [00:16<00:08, 10.12 examples/s]Map (num_proc=32):  69%|██████▉   | 172/250 [00:16<00:08,  9.11 examples/s]Map (num_proc=32):  70%|███████   | 175/250 [00:17<00:07,  9.87 examples/s]Map (num_proc=32):  71%|███████   | 178/250 [00:17<00:05, 12.62 examples/s]Map (num_proc=32):  72%|███████▏  | 180/250 [00:17<00:05, 13.11 examples/s]Map (num_proc=32):  73%|███████▎  | 182/250 [00:17<00:06, 10.39 examples/s]Map (num_proc=32):  74%|███████▎  | 184/250 [00:17<00:06, 10.05 examples/s]Map (num_proc=32):  74%|███████▍  | 186/250 [00:18<00:06,  9.73 examples/s]Map (num_proc=32):  75%|███████▌  | 188/250 [00:18<00:06, 10.33 examples/s]Map (num_proc=32):  76%|███████▌  | 190/250 [00:18<00:05, 10.98 examples/s]Map (num_proc=32):  77%|███████▋  | 192/250 [00:18<00:05, 10.35 examples/s]Map (num_proc=32):  78%|███████▊  | 194/250 [00:18<00:04, 11.65 examples/s]Map (num_proc=32):  79%|███████▉  | 198/250 [00:19<00:03, 13.64 examples/s]Map (num_proc=32):  80%|████████  | 201/250 [00:19<00:03, 13.85 examples/s]Map (num_proc=32):  81%|████████  | 203/250 [00:19<00:03, 13.79 examples/s]Map (num_proc=32):  82%|████████▏ | 205/250 [00:19<00:03, 14.01 examples/s]Map (num_proc=32):  83%|████████▎ | 207/250 [00:19<00:02, 14.51 examples/s]Map (num_proc=32):  84%|████████▍ | 210/250 [00:20<00:04,  9.97 examples/s]Map (num_proc=32):  85%|████████▍ | 212/250 [00:20<00:04,  7.99 examples/s]Map (num_proc=32):  86%|████████▌ | 214/250 [00:20<00:04,  7.81 examples/s]Map (num_proc=32):  87%|████████▋ | 217/250 [00:20<00:03, 10.41 examples/s]Map (num_proc=32):  88%|████████▊ | 219/250 [00:21<00:02, 11.44 examples/s]Map (num_proc=32):  88%|████████▊ | 221/250 [00:21<00:02, 10.90 examples/s]Map (num_proc=32):  90%|█████████ | 225/250 [00:21<00:01, 13.01 examples/s]Map (num_proc=32):  91%|█████████ | 227/250 [00:22<00:03,  7.25 examples/s]Map (num_proc=32):  92%|█████████▏| 229/250 [00:22<00:02,  7.27 examples/s]Map (num_proc=32):  92%|█████████▏| 231/250 [00:22<00:02,  8.36 examples/s]Map (num_proc=32):  93%|█████████▎| 233/250 [00:22<00:01,  9.19 examples/s]Map (num_proc=32):  94%|█████████▍| 235/250 [00:22<00:01,  9.31 examples/s]Map (num_proc=32):  95%|█████████▍| 237/250 [00:23<00:01,  8.64 examples/s]Map (num_proc=32):  95%|█████████▌| 238/250 [00:23<00:01,  6.92 examples/s]Map (num_proc=32):  97%|█████████▋| 242/250 [00:23<00:00, 10.80 examples/s]Map (num_proc=32):  98%|█████████▊| 244/250 [00:24<00:00,  6.16 examples/s]Map (num_proc=32):  98%|█████████▊| 246/250 [00:25<00:01,  3.44 examples/s]Map (num_proc=32):  99%|█████████▉| 248/250 [00:27<00:01,  1.88 examples/s]Map (num_proc=32): 100%|█████████▉| 249/250 [00:31<00:01,  1.07s/ examples]Map (num_proc=32): 100%|██████████| 250/250 [00:46<00:00,  3.61s/ examples]Map (num_proc=32): 100%|██████████| 250/250 [00:46<00:00,  5.42 examples/s]
           score  refusal  non-refusal
jailbreak                             
none       0.012    0.988        0.012
[rank0]:[W125 14:54:08.501329044 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
